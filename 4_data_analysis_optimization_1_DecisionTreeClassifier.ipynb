{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "- This file differs from [2_data_analysis_1_base_data.ipynb](2_data_analysis_1_base_data.ipynb) in that it:\n",
    "    - scales the base cleaned data created in [1_data_cleaning.ipynb](1_data_cleaning.ipynb).\n",
    "\n",
    "Source dataset: 247076 rows × 37 columns\n",
    "Processed and analyzed dataset: 247076 rows × 37 columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports go here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet as fp\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "sys.path.insert(1, 'pkgs')\n",
    "import ml_analysis as mlanlys\n",
    "import ml_clean_feature as mlclean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Read the cleaned dataset from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to results\n",
    "year = 2015\n",
    "source_path     = \"data/\"\n",
    "clean_file      = source_path + 'brfss_' + str(year) + '_clean.parquet.gzip'\n",
    "\n",
    "report_path = 'reports/'\n",
    "performance_report = report_path + 'performance_report.pkl'\n",
    "\n",
    "# BE SURE TO UPDATE THE LABEL FOR THIS ANALYSIS\n",
    "dataset_label = 'RandomUndersampled Dataset'\n",
    "\n",
    "file_label = dataset_label.lower().replace(' ','_')\n",
    "\n",
    "detailed_performance_report = report_path + file_label + '_detailed_performance_report.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read final cleaned dataset from parquet file\n",
    "df = pd.read_parquet(clean_file, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_labels = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253680, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prepare the dataset for analysis\n",
    "\n",
    "- Split the dataset into features and labels.\n",
    "- Split the dataset into training and testing sets.\n",
    "- Scale the dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Dataset Modifications in Process\n",
      "-------------------------------------\n",
      "**Operation:target_column  diabetes\n",
      "**Operation:convert_to_binary  True\n",
      "  -- Converting dataset to binary (0,1) from (0,1,2)\n",
      "\n",
      "\n",
      "****Cleaning Feature: diabetes\n",
      "  Initial Unique features in [diabetes]:  [0. 1. 2.]\n",
      "  values_to_drop: ********* NO Parameters were specified *********\n",
      "  translate: {1: 0, 2: 1}\n",
      "  scale: ********* NO Parameters were specified *********\n",
      "  FINAL Unique features in [diabetes]:  [0. 1.]\n",
      "**Operation:scaler  standard\n",
      "  -- Performing train_test_split on dataframe with target:'diabetes'\n",
      "     -- Run automatically before scalar or random_sample operations\n",
      "  -- Performing StandardScaler on X_train: Updates X_train, y_test\n",
      "**Operation:random_sample  undersample\n",
      "  -- Performing RandomUnderSampler on X_train, y_train: Updates X_train, y_train\n",
      "\n",
      "Dataframe, Train Test Summary\n",
      "-----------------------------\n",
      "Dataframe: (253680, 22)  Data:4, X_train:190260, y_train:190260, X_test:63420, y_test:63420\n",
      "ValueCounts:   y_train: len:2   0: 163538   1: 26722\n",
      "ValueCounts:   y_test : len:2   0:  54796   1:  8624\n"
     ]
    }
   ],
   "source": [
    "# reload any changes to mlanlys\n",
    "importlib.reload(mlanlys)\n",
    "\n",
    "target = 'diabetes'\n",
    "# Dictionary defining modification to be made to the base dataset\n",
    "operation_dict = {  'target_column'     :  target,\n",
    "                    'convert_to_binary' :  True,\n",
    "                    'scaler'            : 'standard', # options: none, standard, minmax\n",
    "                    'random_sample'     : 'undersample'      # options: none, undersample, oversample\n",
    "                    }\n",
    "\n",
    "# This insures that df if not modified during the call to modify_base_dataset()\n",
    "df_modified = df.copy()\n",
    "\n",
    "# Modify the base dataset\n",
    "# data is returned where: X_train, X_test, y_train, y_test = data\n",
    "data = mlanlys.modify_base_dataset(df_modified, operation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe\n",
      "------------------\n",
      "df.shape: (253680, 22)\n",
      "df[diabetes].value_counts:  diabetes\n",
      "0.0    213703\n",
      "2.0     35346\n",
      "1.0      4631\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Modified Dataframe\n",
      "------------------\n",
      "df_modified.shape: (253680, 22)\n",
      "df_modified[diabetes].value_counts:  diabetes\n",
      "0.0    218334\n",
      "1.0     35346\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the original df and the modified dataframe\n",
    "print(f\"Original Dataframe\")\n",
    "print(f\"------------------\")\n",
    "print(f\"df.shape: {df.shape}\")\n",
    "print(f\"df[{target}].value_counts:  {df[target].value_counts()}\")\n",
    "\n",
    "print(f\"\\nModified Dataframe\")\n",
    "print(f\"------------------\")\n",
    "print(f\"df_modified.shape: {df_modified.shape}\")\n",
    "print(f\"df_modified[{target}].value_counts:  {df_modified[target].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe: (253680, 22)  Data:4, X_train:53444, y_train:53444, X_test:63420, y_test:63420\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = data\n",
    "print(f\"Dataframe: {df_modified.shape}  Data:{len(data)}, X_train:{len(X_train)}, y_train:{len(y_train)}, X_test:{len(X_test)}, y_test:{len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diabetes\n",
       "0.0    26722\n",
       "1.0    26722\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diabetes\n",
       "0.0    54796\n",
       "1.0     8624\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Optimization\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1 Pre-optimization metric results\n",
    "\n",
    "- The summary report of the metrics for all pre-optimization runs is here:  [performance_report.txt](reports/performance_report.txt)\n",
    "- The details of the runs are contained in these file:\n",
    "    - [base_dataset_detailed_performance_report.txt](reports/base_dataset_detailed_performance_report.txt)\n",
    "    - [binary_dataset_detailed_performance_report.txt](reports/binary_dataset_detailed_performance_report.txt)\n",
    "    - [minmax_scaled_dataset_detailed_performance_report.txt](reports/minmax_scaled_dataset_detailed_performance_report.txt)\n",
    "    - [randomoversample_dataset_detailed_performance_report.txt](reports/randomoversample_dataset_detailed_performance.txt)\n",
    "    - [cluster_dataset_detailed_performance_report.txt](reports/cluster_dataset_detailed_performance_report.txt)\n",
    "    - [randomundersampled_dataset_detailed_performance_report.txt](reports/randomundersampled_dataset_detailed_performance_report.txt)\n",
    "    - [minmax_scaled_dataset_detailed_performance_report.txt](reports/minmax_scaled_dataset_detailed_performance_report.txt)\n",
    "    - [smoteen_dataset_detailed_performance_report.txt](reports/smoteen_dataset_detailed_performance_report.txt)\n",
    "    - [standard_scaled_dataset_detailed_performance_report.txt](reports/standard_scaled_dataset_detailed_performance_report.txt)\n",
    "    - [smote_dataset_detailed_performance_report.txt](reports/smote_dataset_detailed_performance_report.txt)\n",
    "\n",
    "#### 3.2 Optimization Dataset used\n",
    "\n",
    "**Note:**  Modify the dataset as desired in Section 2.\n",
    "<br><br>\n",
    "Currently the dataset uses the Base dataset for 2015 with the following modifications:\n",
    "- **Target converted to Binary**:  (0,1) from (0,1,2).\n",
    "    - Base:  0: No diabetes, 1: Pre-diabetes, 2: have diabetes\n",
    "    - binary: 0: No diabetes/pre-diabetes, 1: have diabetes\n",
    "- Scaled the data with **StandardScaler**\n",
    "- Resampled the data with **RandomUnderSampler**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  Modify the dataset as desired in Section 2.\n",
    "# Currently the dataset uses the Base dataset for 2015 with the following modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lens, X_train:53444, y_train:53444, X_test:63420, y_test:63420\n",
      "y_train value_counts: diabetes\n",
      "0.0    26722\n",
      "1.0    26722\n",
      "Name: count, dtype: int64\n",
      "y_test value_counts: diabetes\n",
      "0.0    54796\n",
      "1.0     8624\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary:\n",
    "X_train, X_test, y_train, y_test = data\n",
    "print(f\"Dataset Lens, X_train:{len(X_train)}, y_train:{len(y_train)}, X_test:{len(X_test)}, y_test:{len(y_test)}\")\n",
    "print(f\"y_train value_counts: {y_train.value_counts()}\")\n",
    "print(f\"y_test value_counts: {y_test.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Perform Optimization:\n",
    "\n",
    "- Datamodel:  2015 Diabetes Data Set\n",
    "- Optimization approaches: DecisionTreeClassifier RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Decision Tree Regressor: {'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 10}\n",
      "Mean Squared Error of Decision Tree Regressor: 0.29511195206559443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "# param_distributions = {\n",
    "#     'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],  # Depth of the tree\n",
    "#     'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "#     'max_features': ['sqrt', 'log2']  # Number of features to consider for the best split\n",
    "# }\n",
    "\n",
    "parameters = {'criterion':['gini','entropy'],\n",
    "            'max_depth':poisson(mu=2,loc=2),\n",
    "            'min_samples_split':uniform(),\n",
    "            'max_leaf_nodes':poisson(mu=4,loc=3)}\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dtr = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    dtr,\n",
    "    parameters,\n",
    "    cv=2,        # 5-fold cross-validation\n",
    "    n_iter=5,  # Number of parameter settings that are sampled\n",
    "#    verbose=1,   # Verbosity mode\n",
    "    n_jobs=-1,   # Use all available cores\n",
    "    random_state=42  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dtr = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dtr = best_dtr.predict(X_test)\n",
    "\n",
    "data = X_train, y_train, y_pred_dtr\n",
    "\n",
    "# Evaluate the regressor\n",
    "mse_dtr = mean_squared_error(y_test, y_pred_dtr)\n",
    "print(f\"Best Parameters for Decision Tree Regressor: {random_search.best_params_}\")\n",
    "print(f\"Mean Squared Error of Decision Tree Regressor: {mse_dtr}\")\n",
    "\n",
    "best_performance = model_performance(best_dtr, data, 'Best_DecisionTreeClassifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Conclusions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
