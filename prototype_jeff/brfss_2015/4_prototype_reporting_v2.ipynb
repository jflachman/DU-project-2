{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "- This file differs from [2_data_analysis_1_base_data.ipynb](2_data_analysis_1_base_data.ipynb) in that it:\n",
    "    - scales the base cleaned data created in [1_data_cleaning.ipynb](1_data_cleaning.ipynb).\n",
    "\n",
    "Source dataset: 247076 rows × 37 columns\n",
    "Processed and analyzed dataset: 247076 rows × 37 columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:47.200484Z",
     "iopub.status.busy": "2024-06-10T06:18:47.200291Z",
     "iopub.status.idle": "2024-06-10T06:18:48.392608Z",
     "shell.execute_reply": "2024-06-10T06:18:48.391503Z"
    }
   },
   "outputs": [],
   "source": [
    "# package imports go here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastparquet as fp\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import config\n",
    "\n",
    "sys.path.insert(1, config.package_path)\n",
    "import ml_analysis as mlanlys\n",
    "import ml_clean_feature as mlclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Read the cleaned dataset from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.396783Z",
     "iopub.status.busy": "2024-06-10T06:18:48.396255Z",
     "iopub.status.idle": "2024-06-10T06:18:48.418993Z",
     "shell.execute_reply": "2024-06-10T06:18:48.417324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:                        2015\n",
      "Clean File:                  data/brfss_2015_clean.parquet.gzip\n",
      "Performance Report:          reports/performance_report.pkl\n",
      "Detailed Performance Report: reports/5_randomoversampler_dataset_detailed_performance_report.txt\n"
     ]
    }
   ],
   "source": [
    "# reload any changes to Config Settings\n",
    "importlib.reload(config)\n",
    "\n",
    "# BE SURE TO UPDATE THE LABEL FOR THIS ANALYSIS\n",
    "# #############################\n",
    "dataset_label = '5 RandomOverSampler Dataset'\n",
    "# #############################\n",
    "\n",
    "year                        = config.year\n",
    "\n",
    "clean_file                  = config.clean_file\n",
    "performance_report          = config.performance_report\n",
    "\n",
    "report_path                 = config.report_path\n",
    "file_label                  = dataset_label.lower().replace(' ','_')\n",
    "detailed_performance_report = report_path + file_label + '_detailed_performance_report.txt'\n",
    "\n",
    "print(f\"Year:                        {year}\")\n",
    "print(f\"Clean File:                  {clean_file}\")\n",
    "print(f\"Performance Report:          {performance_report}\")\n",
    "print(f\"Detailed Performance Report: {detailed_performance_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.425830Z",
     "iopub.status.busy": "2024-06-10T06:18:48.424031Z",
     "iopub.status.idle": "2024-06-10T06:18:48.807829Z",
     "shell.execute_reply": "2024-06-10T06:18:48.807143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read final cleaned dataset from parquet file\n",
    "df = pd.read_parquet(clean_file, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.810504Z",
     "iopub.status.busy": "2024-06-10T06:18:48.810260Z",
     "iopub.status.idle": "2024-06-10T06:18:48.815200Z",
     "shell.execute_reply": "2024-06-10T06:18:48.813429Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes_labels = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.822215Z",
     "iopub.status.busy": "2024-06-10T06:18:48.820115Z",
     "iopub.status.idle": "2024-06-10T06:18:48.834141Z",
     "shell.execute_reply": "2024-06-10T06:18:48.833033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253680, 22)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prepare the dataset for analysis\n",
    "\n",
    "- Split the dataset into features and labels.\n",
    "- Split the dataset into training and testing sets.\n",
    "- Scale the dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.838377Z",
     "iopub.status.busy": "2024-06-10T06:18:48.837997Z",
     "iopub.status.idle": "2024-06-10T06:18:48.869022Z",
     "shell.execute_reply": "2024-06-10T06:18:48.868172Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:48.871847Z",
     "iopub.status.busy": "2024-06-10T06:18:48.871570Z",
     "iopub.status.idle": "2024-06-10T06:18:49.261172Z",
     "shell.execute_reply": "2024-06-10T06:18:49.260527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Dataset Modifications in Process\n",
      "-------------------------------------\n",
      "**Operation:target_column  diabetes\n",
      "**Operation:convert_to_binary  True\n",
      "  -- Converting dataset to binary (0,1) from (0,1,2)\n",
      "\n",
      "\n",
      "****Cleaning Feature: diabetes\n",
      "  Initial Unique features in [diabetes]:  [0. 1. 2.]\n",
      "  values_to_drop: ********* NO Parameters were specified *********\n",
      "  translate: {1: 0, 2: 1}\n",
      "  scale: ********* NO Parameters were specified *********\n",
      "  FINAL Unique features in [diabetes]:  [0. 1.]\n",
      "**Operation:scaler  standard\n",
      "  -- Performing train_test_split on dataframe with target:'diabetes'\n",
      "     -- Run automatically before scalar or random_sample operations\n",
      "  -- Performing StandardScaler on X_train: Updates X_train, y_test\n",
      "**Operation:random_sample  oversample\n",
      "  -- Performing RandomOverSampler on X_train, y_train: Updates X_train, y_train\n",
      "\n",
      "Dataframe, Train Test Summary\n",
      "-----------------------------\n",
      "Dataframe: (253680, 22)  Data:4, X_train:327526, y_train:327526, X_test:63420, y_test:63420\n",
      "ValueCounts:   y_train: len:2   0: 163763   1:163763\n",
      "ValueCounts:   y_test : len:2   0:  54571   1:  8849\n"
     ]
    }
   ],
   "source": [
    "# reload any changes to mlanlys\n",
    "importlib.reload(mlanlys)\n",
    "\n",
    "target = 'diabetes'\n",
    "# Dictionary defining modification to be made to the base dataset\n",
    "operation_dict = {  'target_column'     :  target,\n",
    "                    'convert_to_binary' :  True,\n",
    "                    'scaler'            : 'standard', # options: none, standard, minmax\n",
    "                    'random_sample'     : 'oversample'      # options: none, undersample, oversample\n",
    "                    }\n",
    "\n",
    "# This insures that df if not modified during the call to modify_base_dataset()\n",
    "df_modified = df.copy()\n",
    "\n",
    "# Modify the base dataset\n",
    "# data is returned where: X_train, X_test, y_train, y_test = data\n",
    "data = mlanlys.modify_base_dataset(df_modified, operation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:49.263837Z",
     "iopub.status.busy": "2024-06-10T06:18:49.263549Z",
     "iopub.status.idle": "2024-06-10T06:18:49.272300Z",
     "shell.execute_reply": "2024-06-10T06:18:49.271468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe\n",
      "------------------\n",
      "df.shape: (253680, 22)\n",
      "df[diabetes].value_counts:  diabetes\n",
      "0.0    213703\n",
      "2.0     35346\n",
      "1.0      4631\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Modified Dataframe\n",
      "------------------\n",
      "df_modified.shape: (253680, 22)\n",
      "df_modified[diabetes].value_counts:  diabetes\n",
      "0.0    218334\n",
      "1.0     35346\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the original df and the modified dataframe\n",
    "print(f\"Original Dataframe\")\n",
    "print(f\"------------------\")\n",
    "print(f\"df.shape: {df.shape}\")\n",
    "print(f\"df[{target}].value_counts:  {df[target].value_counts()}\")\n",
    "\n",
    "print(f\"\\nModified Dataframe\")\n",
    "print(f\"------------------\")\n",
    "print(f\"df_modified.shape: {df_modified.shape}\")\n",
    "print(f\"df_modified[{target}].value_counts:  {df_modified[target].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T06:18:49.274683Z",
     "iopub.status.busy": "2024-06-10T06:18:49.274487Z",
     "iopub.status.idle": "2024-06-10T06:18:49.282118Z",
     "shell.execute_reply": "2024-06-10T06:18:49.281158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe: (253680, 22)  Data:4, X_train:327526, y_train:327526, X_test:63420, y_test:63420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "diabetes\n",
       "0.0    163763\n",
       "1.0    163763\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = data\n",
    "print(f\"Dataframe: {df_modified.shape}  Data:{len(data)}, X_train:{len(X_train)}, y_train:{len(y_train)}, X_test:{len(X_test)}, y_test:{len(y_test)}\")\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Prototype reporting\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_metrics(model, data, datalabel):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score    \n",
    "    # Expand the model data\n",
    "    X, y, y_pred = data\n",
    "\n",
    "    # -------------------------------------- Model Performance\n",
    "    print(f'------------------------------------------------------------------------')\n",
    "    print(f'---------- {datalabel}ing Data Performance\\n------------------------------------------------------------------------')\n",
    "    # -----  Create a confusion matrix\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(f\"Confusion Matrix\\n{conf_mat}\")\n",
    "#        print(f\"Confusion Matrix\\n{confusion_matrix(y, y_pred, labels = [1,0])}\")\n",
    "    \n",
    "    # -----  Score\n",
    "    score_model = round( model.score(X, y), 4)\n",
    "    print(f'\\n-----------------------\\n{datalabel} score: {score_model}')\n",
    "\n",
    "    # -----  Balanced Accuracy\n",
    "    score_ba = round( balanced_accuracy_score(y, y_pred), 4)\n",
    "    print(f\"Balanced Accuracy Score: {score_ba}\")\n",
    "\n",
    "    # -----  ROC AUC Score\n",
    "    if len(y.value_counts())>2:\n",
    "        score_roc_auc = round( roc_auc_score(y, model.predict_proba(X), multi_class='ovr'), 4)\n",
    "    else:\n",
    "        score_roc_auc = round( roc_auc_score(y, model.predict_proba(X)[:, 1]), 4)\n",
    "\n",
    "    print(f\"ROC AUC Score: {score_roc_auc}\")\n",
    "\n",
    "\n",
    "    mse = round( mean_squared_error(y, y_pred), 4)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    # -----  Create a classification report\n",
    "    print(f\"\\n-----------------------\\nClassification Report\\n{classification_report(y, y_pred)}\")\n",
    "\n",
    "    # ----- metrics caluculated from the Confusion Matrix\n",
    "    # Unravel the confusion matrix\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "\n",
    "    print(f\"------------------------------\")\n",
    "    print(f\"--- Classification values\")\n",
    "    print(f\"------------------------------\")\n",
    "    # Computer acuracy\n",
    "    accuracy = round( ( (tp + tn) / (tp + tn + fp + fn) ),4)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    precision = round( (tp / (tp + fp) ),4)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    recall = round( (tp / (tp + fn) ),4)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    f1 = round( (2 * (precision * recall) / (precision + recall) ),4)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    specificity = round( (tn / (tn + fp) ),4)\n",
    "    print(\"Specificity:\", specificity)\n",
    "\n",
    "    fpr = round( (fp / (fp + tn) ),4)\n",
    "    print(\"False Positive Rate:\", fpr)\n",
    "\n",
    "    mcc = round( ( (tp * tn - fp * fn) / (((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5) ),4)\n",
    "    print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "    return {'model': type(model).__name__, \n",
    "            'slice': datalabel,\n",
    "            'score':score_model, \n",
    "            'balanced_accuracy': score_ba, \n",
    "            'roc_auc_score':score_roc_auc,\n",
    "            \"Mean Squared Error\": mse,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Specificity': specificity,\n",
    "            'False Positive Rate': fpr,\n",
    "            'Matthews Correlation Coefficient': mcc\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ml_analysis' from '/mnt/c/ML/DU/repos/projects/project-2/DU-project-2-2015/prototype_jeff/brfss_2015/../../pkgs/ml_analysis.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload any changes to mlanlys\n",
    "importlib.reload(mlanlys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: X_train, X_test, y_train, y_test = data already performed at the end of step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "# Train the model\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "test_data = [ X_test, y_test, y_test_pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "---------- Original Decision Tree Classifiering Data Performance\n",
      "------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "[[48017  6554]\n",
      " [ 6273  2576]]\n",
      "\n",
      "-----------------------\n",
      "Original Decision Tree Classifier score: 0.7977\n",
      "Balanced Accuracy Score: 0.5855\n",
      "ROC AUC Score: 0.5861\n",
      "Mean Squared Error: 0.2023\n",
      "------------------------------\n",
      "--- Classification values\n",
      "------------------------------\n",
      "Accuracy: 0.7977\n",
      "Precision: 0.2821\n",
      "Recall: 0.2911\n",
      "F1-score: 0.2865\n",
      "Specificity: 0.8799\n",
      "False Positive Rate: 0.1201\n",
      "Matthews Correlation Coefficient: 0.1688\n",
      "\n",
      "-----------------------\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88     54571\n",
      "         1.0       0.28      0.29      0.29      8849\n",
      "\n",
      "    accuracy                           0.80     63420\n",
      "   macro avg       0.58      0.59      0.58     63420\n",
      "weighted avg       0.80      0.80      0.80     63420\n",
      "\n",
      "{'model': 'DecisionTreeClassifier', 'slice': 'Original Decision Tree Classifier', 'score': 0.7977, 'balanced_accuracy': 0.5855, 'roc_auc_score': 0.5861, 'Mean Squared Error': 0.2023, 'Accuracy': 0.7977, 'Precision': 0.2821, 'Recall': 0.2911, 'F1-score': 0.2865, 'Specificity': 0.8799, 'False Positive Rate': 0.1201, 'Matthews Correlation Coefficient': 0.1688}\n"
     ]
    }
   ],
   "source": [
    "# reload any changes to mlanlys\n",
    "importlib.reload(mlanlys)\n",
    "\n",
    "mymetrics = mlanlys.model_performance_metrics(model, test_data, \"Original Decision Tree Classifier\" )\n",
    "\n",
    "print(mymetrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
